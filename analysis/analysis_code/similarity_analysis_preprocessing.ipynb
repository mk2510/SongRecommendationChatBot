{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (1.21.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from click->nltk) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (4.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: texthero in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (1.3.4)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (1.21.3)\n",
      "Requirement already satisfied: gensim<4.0,>=3.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (3.8.3)\n",
      "Requirement already satisfied: spacy<3.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (2.3.7)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (1.0.1)\n",
      "Requirement already satisfied: tqdm>=4.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (4.63.1)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (5.7.0)\n",
      "Requirement already satisfied: nltk>=3.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (3.7)\n",
      "Requirement already satisfied: wordcloud>=1.5.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (1.8.1)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from texthero) (1.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (8.4.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.27.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.9.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (47.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.7.7)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (7.4.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (1.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk>=3.3->texthero) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk>=3.3->texthero) (8.1.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2021.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.21.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (4.17.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: scipy in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: nltk in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: torchvision in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (4.63.1)\n",
      "Requirement already satisfied: requests in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: click in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: six in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZqVO-tI_aRu",
    "outputId": "4700d44a-9b12-4479-9f66-97f900c22482"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow_hub as hub\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "import texthero as hero\n",
    "from sentence_transformers import SentenceTransformer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZb5GgZ3CNY5"
   },
   "source": [
    "here we generate the dataframe, which keyword named as title and GT recommendation named as content.\n",
    "\n",
    "We are using here just the NYT articles, as buy using articles from just one publisher we hope to get a better more consistant matching and less confusion, which is important, when building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ud_VKIQAAJuj",
    "outputId": "4c57ecd2-b6d0-4c6d-e8d7-31df4961d81c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>U.N. Relief Official Calls Crisis in Aleppo th...</td>\n",
       "      <td>The top aid official at the United Nations gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>Federal Judge Curbs Enforcement of North Carol...</td>\n",
       "      <td>A federal judge on Friday curbed the enforceme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>Mexicans Accuse President of ‘Historic Error’ ...</td>\n",
       "      <td>MEXICO CITY  —   If President Enrique Peña Nie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7801</th>\n",
       "      <td>U.S. Presses for Truce in Syria, With Its Larg...</td>\n",
       "      <td>HANGZHOU, China  —   The image of a    Syrian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>Airbnb Adopts Rules to Fight Discrimination by...</td>\n",
       "      <td>SAN FRANCISCO  —   For much of this year, Airb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7803 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     House Republicans Fret About Winning Their Hea...   \n",
       "1     Rift Between Officers and Residents as Killing...   \n",
       "2     Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3     Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4     Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "...                                                 ...   \n",
       "7798  U.N. Relief Official Calls Crisis in Aleppo th...   \n",
       "7799  Federal Judge Curbs Enforcement of North Carol...   \n",
       "7800  Mexicans Accuse President of ‘Historic Error’ ...   \n",
       "7801  U.S. Presses for Truce in Syria, With Its Larg...   \n",
       "7802  Airbnb Adopts Rules to Fight Discrimination by...   \n",
       "\n",
       "                                                content  \n",
       "0     WASHINGTON  —   Congressional Republicans have...  \n",
       "1     After the bullet shells get counted, the blood...  \n",
       "2     When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3     Death may be the great equalizer, but it isn’t...  \n",
       "4     SEOUL, South Korea  —   North Korea’s leader, ...  \n",
       "...                                                 ...  \n",
       "7798  The top aid official at the United Nations gav...  \n",
       "7799  A federal judge on Friday curbed the enforceme...  \n",
       "7800  MEXICO CITY  —   If President Enrique Peña Nie...  \n",
       "7801  HANGZHOU, China  —   The image of a    Syrian ...  \n",
       "7802  SAN FRANCISCO  —   For much of this year, Airb...  \n",
       "\n",
       "[7803 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../comparison_dataset/articles1.csv')\n",
    "df = df[df['publication'] == 'New York Times']\n",
    "df = df.drop(columns = ['id', 'publication', 'author', 'date', 'year', 'month', 'url', 'Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9krNii0CWUC"
   },
   "source": [
    "now we try to find embedding methods, which in an ideal matching, or we could also just try to minimize distance between the embedded title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = hero.clean(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D02XfBTKCK7z"
   },
   "outputs": [],
   "source": [
    "def embedding_by_doc_2_vec(df):\n",
    "  data = df['content']\n",
    "  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "  # df['tagged_data'] = tagged_data\n",
    "  max_epochs = 5\n",
    "  vec_size = 20\n",
    "  alpha = 0.025\n",
    "\n",
    "  model = Doc2Vec(#size = vec_size,\n",
    "    vector_size=vec_size,\n",
    "                  alpha=alpha, \n",
    "                  min_alpha=0.00025,\n",
    "                  min_count=1,\n",
    "                  dm =1)\n",
    "    \n",
    "  model.build_vocab(tagged_data)\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "      print('iteration {0}'.format(epoch))\n",
    "      model.train(tagged_data,\n",
    "                  total_examples=model.corpus_count,\n",
    "                  #epochs = model.iter\n",
    "                  epochs=model.epochs)\n",
    "      # decrease the learning rate\n",
    "      model.alpha -= 0.0002\n",
    "      # fix the learning rate, no decay\n",
    "      model.min_alpha = model.alpha\n",
    "  print('here')\n",
    "  print(model.docvecs['1'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "xCqXk2VHFBgG",
    "outputId": "0f80c9eb-d7ed-41c1-e948-04b4490c12e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/3908529724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_d2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_by_doc_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/3047116187.py\u001b[0m in \u001b[0;36membedding_by_doc_2_vec\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     21\u001b[0m                   \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                   \u001b[0;31m#epochs = model.iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                   epochs=model.epochs)\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;31m# decrease the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.0002\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    486\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_d2v = embedding_by_doc_2_vec(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_d2v.save(\"doc2vec_clean.model\")\n",
    "#model_d2v = Doc2Vec.load_doc(\"doc2vec_clean.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>embed_title_d2v</th>\n",
       "      <th>embed_content_d2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>washington -- congressional republicans new fe...</td>\n",
       "      <td>[0.06387405, 0.0027298941, -0.07726031, 0.0393...</td>\n",
       "      <td>[-0.1479157, 2.0191174, 1.9422089, -1.2744783,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>bullet shells get counted blood dries votive c...</td>\n",
       "      <td>[0.05206804, 0.035272043, -0.025424976, -0.022...</td>\n",
       "      <td>[8.846913, -1.4289446, 3.6064503, -2.3176916, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>walt disney' \"bambi\" opened critics praised sp...</td>\n",
       "      <td>[0.064312935, 0.009777807, -0.016095767, -0.01...</td>\n",
       "      <td>[1.1507585, -0.75324535, -3.7713225, -3.423285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>death may great equalizer ' necessarily evenha...</td>\n",
       "      <td>[0.06839463, 0.01517621, -0.040933907, -0.0239...</td>\n",
       "      <td>[1.8920553, -0.74019766, -2.7524686, 1.0556654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>seoul south korea -- north korea' leader kim s...</td>\n",
       "      <td>[0.12724127, -0.01452525, -0.05128109, -0.0053...</td>\n",
       "      <td>[-0.9053595, 1.3270155, -0.028560245, 1.839707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>U.N. Relief Official Calls Crisis in Aleppo th...</td>\n",
       "      <td>top aid official united nations gave gloomy as...</td>\n",
       "      <td>[0.12453497, -0.04603657, -0.029953754, 0.0265...</td>\n",
       "      <td>[-0.08758663, 0.97439015, 0.8159305, 0.1262278...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>Federal Judge Curbs Enforcement of North Carol...</td>\n",
       "      <td>federal judge friday curbed enforcement north ...</td>\n",
       "      <td>[0.066655576, -0.0291929, -0.0071223252, -0.01...</td>\n",
       "      <td>[2.723682, 1.3773351, -0.05944748, 0.9979264, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>Mexicans Accuse President of ‘Historic Error’ ...</td>\n",
       "      <td>mexico city -- president enrique pena nieto in...</td>\n",
       "      <td>[0.118740484, -0.028534405, -0.044770654, -0.0...</td>\n",
       "      <td>[-0.70808154, 2.3987489, -2.6012354, 1.5189507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7801</th>\n",
       "      <td>U.S. Presses for Truce in Syria, With Its Larg...</td>\n",
       "      <td>hangzhou china -- image syrian boy dazed blood...</td>\n",
       "      <td>[0.07130332, -0.02886117, -0.044939496, -0.033...</td>\n",
       "      <td>[-3.2483962, 1.4409665, 1.1384422, 1.4458574, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>Airbnb Adopts Rules to Fight Discrimination by...</td>\n",
       "      <td>san francisco -- much year airbnb fire ease ho...</td>\n",
       "      <td>[0.04321503, -0.038976744, -0.07799573, -0.049...</td>\n",
       "      <td>[3.0400224, 3.3972666, 3.250455, -1.2224009, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7803 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     House Republicans Fret About Winning Their Hea...   \n",
       "1     Rift Between Officers and Residents as Killing...   \n",
       "2     Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3     Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4     Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "...                                                 ...   \n",
       "7798  U.N. Relief Official Calls Crisis in Aleppo th...   \n",
       "7799  Federal Judge Curbs Enforcement of North Carol...   \n",
       "7800  Mexicans Accuse President of ‘Historic Error’ ...   \n",
       "7801  U.S. Presses for Truce in Syria, With Its Larg...   \n",
       "7802  Airbnb Adopts Rules to Fight Discrimination by...   \n",
       "\n",
       "                                                content  \\\n",
       "0     washington -- congressional republicans new fe...   \n",
       "1     bullet shells get counted blood dries votive c...   \n",
       "2     walt disney' \"bambi\" opened critics praised sp...   \n",
       "3     death may great equalizer ' necessarily evenha...   \n",
       "4     seoul south korea -- north korea' leader kim s...   \n",
       "...                                                 ...   \n",
       "7798  top aid official united nations gave gloomy as...   \n",
       "7799  federal judge friday curbed enforcement north ...   \n",
       "7800  mexico city -- president enrique pena nieto in...   \n",
       "7801  hangzhou china -- image syrian boy dazed blood...   \n",
       "7802  san francisco -- much year airbnb fire ease ho...   \n",
       "\n",
       "                                        embed_title_d2v  \\\n",
       "0     [0.06387405, 0.0027298941, -0.07726031, 0.0393...   \n",
       "1     [0.05206804, 0.035272043, -0.025424976, -0.022...   \n",
       "2     [0.064312935, 0.009777807, -0.016095767, -0.01...   \n",
       "3     [0.06839463, 0.01517621, -0.040933907, -0.0239...   \n",
       "4     [0.12724127, -0.01452525, -0.05128109, -0.0053...   \n",
       "...                                                 ...   \n",
       "7798  [0.12453497, -0.04603657, -0.029953754, 0.0265...   \n",
       "7799  [0.066655576, -0.0291929, -0.0071223252, -0.01...   \n",
       "7800  [0.118740484, -0.028534405, -0.044770654, -0.0...   \n",
       "7801  [0.07130332, -0.02886117, -0.044939496, -0.033...   \n",
       "7802  [0.04321503, -0.038976744, -0.07799573, -0.049...   \n",
       "\n",
       "                                      embed_content_d2v  \n",
       "0     [-0.1479157, 2.0191174, 1.9422089, -1.2744783,...  \n",
       "1     [8.846913, -1.4289446, 3.6064503, -2.3176916, ...  \n",
       "2     [1.1507585, -0.75324535, -3.7713225, -3.423285...  \n",
       "3     [1.8920553, -0.74019766, -2.7524686, 1.0556654...  \n",
       "4     [-0.9053595, 1.3270155, -0.028560245, 1.839707...  \n",
       "...                                                 ...  \n",
       "7798  [-0.08758663, 0.97439015, 0.8159305, 0.1262278...  \n",
       "7799  [2.723682, 1.3773351, -0.05944748, 0.9979264, ...  \n",
       "7800  [-0.70808154, 2.3987489, -2.6012354, 1.5189507...  \n",
       "7801  [-3.2483962, 1.4409665, 1.1384422, 1.4458574, ...  \n",
       "7802  [3.0400224, 3.3972666, 3.250455, -1.2224009, 0...  \n",
       "\n",
       "[7803 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedd_d2v(model, doc):\n",
    "    return model.infer_vector(doc.split())\n",
    "\n",
    "df['embed_title_d2v'] = df['title'].apply(lambda x: embedd_d2v(model_d2v, x))\n",
    "df['embed_content_d2v'] = df['content'].apply(lambda x: embedd_d2v(model_d2v, x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(df):\n",
    "    return 1 - spatial.distance.cosine(df[0], df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39202946531022015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['d2v_similarity'] = df[['embed_title_d2v', 'embed_content_d2v']].apply(cosine_similarity, axis = 1)\n",
    "df['d2v_similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (Mean:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/3627148274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_title_USE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membed_with_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_USE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_content_USE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membed_with_USE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_USE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USE_similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_title_USE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embed_content_USE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USE_similarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/4001122713.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# clamp the result to 0-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \"\"\"\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_vector\u001b[0;34m(u, dtype)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_validate_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m# XXX Is order='c' really necessary?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (Mean:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "embed_USE = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def embed_with_USE(embedder , x):\n",
    "    return tf.reduce_mean(embedder(x.split('.')),0)\n",
    "\n",
    "df['embed_title_USE'] = df['title'].apply(lambda x: embed_with_USE(embed_USE, x))\n",
    "df['embed_content_USE'] = df['content'].apply(lambda x: embed_with_USE(embed_USE, x))\n",
    "df['USE_similarity'] = df[['embed_title_USE', 'embed_content_USE']].apply(cosine_similarity, axis = 1)\n",
    "df['USE_similarity'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456832346447217"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BERT = SentenceTransformer(r\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_with_USE(model , x):\n",
    "    return model.encode(x)\n",
    "\n",
    "df['embed_title_BERT'] = df['title'].apply(lambda x: embed_with_USE(model_BERT, x))\n",
    "df['embed_content_BERT'] = df['content'].apply(lambda x: embed_with_USE(model_BERT, x))\n",
    "df['BERT_similarity'] = df[['embed_title_BERT', 'embed_content_BERT']].apply(cosine_similarity, axis = 1)\n",
    "df['BERT_similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 1.15.0\n",
      "Uninstalling tensorflow-1.15.0:\n",
      "  Successfully uninstalled tensorflow-1.15.0\n",
      "\u001b[33mWARNING: Skipping tensorflow-cloud as it is not installed.\u001b[0m\n",
      "Collecting tensorflow==1.15\n",
      "  Using cached tensorflow-1.15.0-cp37-cp37m-macosx_10_11_x86_64.whl (124.0 MB)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (3.20.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.44.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (47.1.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
      "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-1.15.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip uninstall tensorflow-cloud -y\n",
    "!pip install -U tensorflow==1.15\n",
    "!pip uninstall pytorch-lightning -y\n",
    "!pip uninstall tensorflow-probability -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:10: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:10: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:11: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 19:42:03.989966: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-04-16 19:42:04.041330: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fde2d3f2150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-16 19:42:04.041346: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:11: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:12: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/405206852.py:12: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")\n",
    "\n",
    "def create_elmo_embeddings(data):\n",
    "    return_ls = []\n",
    "    for dat in data.tolist():\n",
    "        embed=elmo.signatures['default']( tf.convert_to_tensor([dat]))[\"elmo\"]\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            out_x=sess.run(embed)\n",
    "            #out_y=ses.run(tf.reduce_mean(embed,1))\n",
    "        return_ls.append(out_x)\n",
    "    return return_ls\n",
    "\n",
    "temp1 = create_elmo_embeddings(df['content'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/3574872249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m embeddings = elmo(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5717\u001b[0m         \u001b[0mdevice_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5718\u001b[0m         \u001b[0mexecution_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5719\u001b[0;31m         server_def=None)\n\u001b[0m\u001b[1;32m   5720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[0;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[1;32m   5776\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5777\u001b[0m       raise ValueError(\n\u001b[0;32m-> 5778\u001b[0;31m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5779\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5780\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "elmo = tf_hub.Module(\"https://tfhub.dev/google/elmo/2\")\n",
    "embeddings = elmo(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/1204370908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_content_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvdT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_title_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvdT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf_similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embed_title_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embed_content_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf_similarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cosine_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "max_feat = 10000\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5, max_features=max_feat, ngram_range=(1,3),\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english')\n",
    "svdT = TruncatedSVD(n_components=390)\n",
    "tfidf.fit(df['content'])\n",
    "df['embed_content_tfidf'] = svdT.fit_transform(tfidf.transform(df['content'])).tolist()\n",
    "df['embed_title_tfidf'] = svdT.fit_transform(tfidf.transform(df['title'])).tolist()\n",
    "df['tfidf_similarity'] = df[['embed_title_tfidf', 'embed_content_tfidf']].apply(cosine_similarity, axis = 1)\n",
    "df['tfidf_similarity'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import os\n",
    "root_folder='.'\n",
    "data_folder_name='data'\n",
    "\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "\n",
    "glove_filename='glove.6B.100d.txt'\n",
    "#glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
    "#glove_input_file = glove_filename\n",
    "#word2vec_output_file = glove_filename+'.word2vec'\n",
    "#glove2word2vec(glove_path, word2vec_output_file)\n",
    "\n",
    "# load the Stanford GloVe model\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: Mean of empty slice.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8266875235549795"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def embed_glove(s):\n",
    "    return_ls = []\n",
    "    for sentence in s:\n",
    "        array = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                array.append(model.get_vector(word))\n",
    "            except:\n",
    "                pass\n",
    "        array = np.array(array)\n",
    "        #print(array.shape)\n",
    "        return_ls.append(array.mean(axis = 0))\n",
    "\n",
    "    return return_ls\n",
    "#Show a word embedding\n",
    "df['embed_content_glove'] = embed_glove(df['content'])\n",
    "df['embed_title_glove'] = embed_glove(df['title'])\n",
    "df['glove_similarity'] = df[['embed_title_glove', 'embed_content_glove']].apply(cosine_similarity, axis = 1)\n",
    "df['glove_similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ff/v8q71qfn4hbdkzbpmf28ymsr0000gn/T/ipykernel_3795/4097391222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha)\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    486\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = df['content']\n",
    "sentences = [s.split() for s in sentences]\n",
    "model = FastText(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fasttext(s):\n",
    "    return_ls = []\n",
    "    for sentence in s:\n",
    "        array = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                array.append(model.wv[word])\n",
    "            except:\n",
    "                pass\n",
    "        array = np.array(array)\n",
    "        #print(array.shape)\n",
    "        return_ls.append(array.mean(axis = 0))\n",
    "\n",
    "    return return_ls\n",
    "\n",
    "df['embed_content_fasttext'] = embed_fasttext(df['content'])\n",
    "df['embed_title_fasttext'] = embed_fasttext(df['title'])\n",
    "df['fasttext_similarity'] = df[['embed_title_fasttext', 'embed_content_fasttext']].apply(cosine_similarity, axis = 1)\n",
    "df['fasttext_similarity'].mean()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
