{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (1.21.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from gensim) (1.7.1)\n",
            "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (4.63.1)\n",
            "Requirement already satisfied: click in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (8.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (2022.3.15)\n",
            "Requirement already satisfied: joblib in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from click->nltk) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.6.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install texthero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (2.2.0)\n",
            "Requirement already satisfied: scipy in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: huggingface-hub in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (1.21.3)\n",
            "Requirement already satisfied: tqdm in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (4.63.1)\n",
            "Requirement already satisfied: torchvision in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (0.12.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (4.17.0)\n",
            "Requirement already satisfied: nltk in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: filelock in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: sacremoses in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.11.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.3.15)\n",
            "Requirement already satisfied: click in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->huggingface-hub->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->huggingface-hub->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->huggingface-hub->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from requests->huggingface-hub->sentence-transformers) (1.26.9)\n",
            "Requirement already satisfied: six in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZqVO-tI_aRu",
        "outputId": "4700d44a-9b12-4479-9f66-97f900c22482"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/max/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import tensorflow_hub as hub\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy import spatial\n",
        "import nltk\n",
        "import texthero as hero\n",
        "from sentence_transformers import SentenceTransformer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZb5GgZ3CNY5"
      },
      "source": [
        "here we generate the dataframe, which keyword named as title and GT recommendation named as content.\n",
        "\n",
        "We are using here just the NYT articles, as buy using articles from just one publisher we hope to get a better more consistant matching and less confusion, which is important, when building the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ud_VKIQAAJuj",
        "outputId": "4c57ecd2-b6d0-4c6d-e8d7-31df4961d81c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7798</th>\n",
              "      <td>U.N. Relief Official Calls Crisis in Aleppo th...</td>\n",
              "      <td>The top aid official at the United Nations gav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7799</th>\n",
              "      <td>Federal Judge Curbs Enforcement of North Carol...</td>\n",
              "      <td>A federal judge on Friday curbed the enforceme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7800</th>\n",
              "      <td>Mexicans Accuse President of ‘Historic Error’ ...</td>\n",
              "      <td>MEXICO CITY  —   If President Enrique Peña Nie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7801</th>\n",
              "      <td>U.S. Presses for Truce in Syria, With Its Larg...</td>\n",
              "      <td>HANGZHOU, China  —   The image of a    Syrian ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7802</th>\n",
              "      <td>Airbnb Adopts Rules to Fight Discrimination by...</td>\n",
              "      <td>SAN FRANCISCO  —   For much of this year, Airb...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7803 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  \\\n",
              "0     House Republicans Fret About Winning Their Hea...   \n",
              "1     Rift Between Officers and Residents as Killing...   \n",
              "2     Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
              "3     Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
              "4     Kim Jong-un Says North Korea Is Preparing to T...   \n",
              "...                                                 ...   \n",
              "7798  U.N. Relief Official Calls Crisis in Aleppo th...   \n",
              "7799  Federal Judge Curbs Enforcement of North Carol...   \n",
              "7800  Mexicans Accuse President of ‘Historic Error’ ...   \n",
              "7801  U.S. Presses for Truce in Syria, With Its Larg...   \n",
              "7802  Airbnb Adopts Rules to Fight Discrimination by...   \n",
              "\n",
              "                                                content  \n",
              "0     WASHINGTON  —   Congressional Republicans have...  \n",
              "1     After the bullet shells get counted, the blood...  \n",
              "2     When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
              "3     Death may be the great equalizer, but it isn’t...  \n",
              "4     SEOUL, South Korea  —   North Korea’s leader, ...  \n",
              "...                                                 ...  \n",
              "7798  The top aid official at the United Nations gav...  \n",
              "7799  A federal judge on Friday curbed the enforceme...  \n",
              "7800  MEXICO CITY  —   If President Enrique Peña Nie...  \n",
              "7801  HANGZHOU, China  —   The image of a    Syrian ...  \n",
              "7802  SAN FRANCISCO  —   For much of this year, Airb...  \n",
              "\n",
              "[7803 rows x 2 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('../comparison_dataset/articles1.csv')\n",
        "df = df[df['publication'] == 'New York Times']\n",
        "df = df.drop(columns = ['id', 'publication', 'author', 'date', 'year', 'month', 'url', 'Unnamed: 0'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9krNii0CWUC"
      },
      "source": [
        "now we try to find embedding methods, which in an ideal matching, or we could also just try to minimize distance between the embedded title and content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['content'] = hero.clean(df['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D02XfBTKCK7z"
      },
      "outputs": [],
      "source": [
        "def embedding_by_doc_2_vec(df):\n",
        "  data = df['content']\n",
        "  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
        "  # df['tagged_data'] = tagged_data\n",
        "  max_epochs = 5\n",
        "  vec_size = 20\n",
        "  alpha = 0.025\n",
        "\n",
        "  model = Doc2Vec(#size = vec_size,\n",
        "    vector_size=vec_size,\n",
        "                  alpha=alpha, \n",
        "                  min_alpha=0.00025,\n",
        "                  min_count=1,\n",
        "                  dm =1)\n",
        "    \n",
        "  model.build_vocab(tagged_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "      print('iteration {0}'.format(epoch))\n",
        "      model.train(tagged_data,\n",
        "                  total_examples=model.corpus_count,\n",
        "                  #epochs = model.iter\n",
        "                  epochs=model.epochs)\n",
        "      # decrease the learning rate\n",
        "      model.alpha -= 0.0002\n",
        "      # fix the learning rate, no decay\n",
        "      model.min_alpha = model.alpha\n",
        "  print('here')\n",
        "  print(model.docvecs['1'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "xCqXk2VHFBgG",
        "outputId": "0f80c9eb-d7ed-41c1-e948-04b4490c12e0"
      },
      "outputs": [],
      "source": [
        "#model_d2v = embedding_by_doc_2_vec(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model_d2v.save(\"doc2vec.model\")\n",
        "model_d2v = Doc2Vec.load(\"doc2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>embed_title_d2v</th>\n",
              "      <th>embed_content_d2v</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "      <td>[-0.00020618885, 0.0017079205, 0.012472818, -0...</td>\n",
              "      <td>[-0.48268387, -0.037932932, -0.84915787, 5.011...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "      <td>[-0.051277727, 0.3151049, 0.024677651, -0.0277...</td>\n",
              "      <td>[3.0910127, 0.4342424, -0.098031476, 0.1685604...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "      <td>[-0.04005891, 0.54228365, 0.06120843, -0.02935...</td>\n",
              "      <td>[1.2332672, 2.1408443, 2.617472, -3.2744303, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "      <td>[-0.075641185, 0.1533827, 0.02122116, 0.104032...</td>\n",
              "      <td>[-1.9783455, 2.961961, 6.096002, -0.82950646, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "      <td>[-0.071448535, 0.03868655, 0.010632665, -0.010...</td>\n",
              "      <td>[0.69137114, 3.069012, 3.005353, 2.6914027, 5....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7798</th>\n",
              "      <td>U.N. Relief Official Calls Crisis in Aleppo th...</td>\n",
              "      <td>The top aid official at the United Nations gav...</td>\n",
              "      <td>[-0.063712195, 0.20381376, 0.03313346, 0.06360...</td>\n",
              "      <td>[1.4032047, 1.8695139, 0.8486812, 1.3467678, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7799</th>\n",
              "      <td>Federal Judge Curbs Enforcement of North Carol...</td>\n",
              "      <td>A federal judge on Friday curbed the enforceme...</td>\n",
              "      <td>[0.022196058, 0.022633335, -0.01483866, 0.0132...</td>\n",
              "      <td>[0.24696188, -2.430637, -2.117606, 2.1944127, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7800</th>\n",
              "      <td>Mexicans Accuse President of ‘Historic Error’ ...</td>\n",
              "      <td>MEXICO CITY  —   If President Enrique Peña Nie...</td>\n",
              "      <td>[-0.014848642, 0.005586029, -0.007927232, 0.00...</td>\n",
              "      <td>[0.4819322, 2.1651163, 0.16808361, 2.2401412, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7801</th>\n",
              "      <td>U.S. Presses for Truce in Syria, With Its Larg...</td>\n",
              "      <td>HANGZHOU, China  —   The image of a    Syrian ...</td>\n",
              "      <td>[-0.026488753, 0.35839713, -0.010913456, 0.044...</td>\n",
              "      <td>[0.65575665, 2.396529, 1.9432513, 3.6524916, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7802</th>\n",
              "      <td>Airbnb Adopts Rules to Fight Discrimination by...</td>\n",
              "      <td>SAN FRANCISCO  —   For much of this year, Airb...</td>\n",
              "      <td>[-0.095118046, 0.20087585, -0.0020927377, 0.02...</td>\n",
              "      <td>[1.5429983, 2.1652133, -4.9059453, 1.9236624, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7803 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  \\\n",
              "0     House Republicans Fret About Winning Their Hea...   \n",
              "1     Rift Between Officers and Residents as Killing...   \n",
              "2     Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
              "3     Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
              "4     Kim Jong-un Says North Korea Is Preparing to T...   \n",
              "...                                                 ...   \n",
              "7798  U.N. Relief Official Calls Crisis in Aleppo th...   \n",
              "7799  Federal Judge Curbs Enforcement of North Carol...   \n",
              "7800  Mexicans Accuse President of ‘Historic Error’ ...   \n",
              "7801  U.S. Presses for Truce in Syria, With Its Larg...   \n",
              "7802  Airbnb Adopts Rules to Fight Discrimination by...   \n",
              "\n",
              "                                                content  \\\n",
              "0     WASHINGTON  —   Congressional Republicans have...   \n",
              "1     After the bullet shells get counted, the blood...   \n",
              "2     When Walt Disney’s “Bambi” opened in 1942, cri...   \n",
              "3     Death may be the great equalizer, but it isn’t...   \n",
              "4     SEOUL, South Korea  —   North Korea’s leader, ...   \n",
              "...                                                 ...   \n",
              "7798  The top aid official at the United Nations gav...   \n",
              "7799  A federal judge on Friday curbed the enforceme...   \n",
              "7800  MEXICO CITY  —   If President Enrique Peña Nie...   \n",
              "7801  HANGZHOU, China  —   The image of a    Syrian ...   \n",
              "7802  SAN FRANCISCO  —   For much of this year, Airb...   \n",
              "\n",
              "                                        embed_title_d2v  \\\n",
              "0     [-0.00020618885, 0.0017079205, 0.012472818, -0...   \n",
              "1     [-0.051277727, 0.3151049, 0.024677651, -0.0277...   \n",
              "2     [-0.04005891, 0.54228365, 0.06120843, -0.02935...   \n",
              "3     [-0.075641185, 0.1533827, 0.02122116, 0.104032...   \n",
              "4     [-0.071448535, 0.03868655, 0.010632665, -0.010...   \n",
              "...                                                 ...   \n",
              "7798  [-0.063712195, 0.20381376, 0.03313346, 0.06360...   \n",
              "7799  [0.022196058, 0.022633335, -0.01483866, 0.0132...   \n",
              "7800  [-0.014848642, 0.005586029, -0.007927232, 0.00...   \n",
              "7801  [-0.026488753, 0.35839713, -0.010913456, 0.044...   \n",
              "7802  [-0.095118046, 0.20087585, -0.0020927377, 0.02...   \n",
              "\n",
              "                                      embed_content_d2v  \n",
              "0     [-0.48268387, -0.037932932, -0.84915787, 5.011...  \n",
              "1     [3.0910127, 0.4342424, -0.098031476, 0.1685604...  \n",
              "2     [1.2332672, 2.1408443, 2.617472, -3.2744303, 0...  \n",
              "3     [-1.9783455, 2.961961, 6.096002, -0.82950646, ...  \n",
              "4     [0.69137114, 3.069012, 3.005353, 2.6914027, 5....  \n",
              "...                                                 ...  \n",
              "7798  [1.4032047, 1.8695139, 0.8486812, 1.3467678, 3...  \n",
              "7799  [0.24696188, -2.430637, -2.117606, 2.1944127, ...  \n",
              "7800  [0.4819322, 2.1651163, 0.16808361, 2.2401412, ...  \n",
              "7801  [0.65575665, 2.396529, 1.9432513, 3.6524916, 4...  \n",
              "7802  [1.5429983, 2.1652133, -4.9059453, 1.9236624, ...  \n",
              "\n",
              "[7803 rows x 4 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def embedd_d2v(model, doc):\n",
        "    return model.infer_vector(doc.split())\n",
        "\n",
        "df['embed_title_d2v'] = df['title'].apply(lambda x: embedd_d2v(model_d2v, x))\n",
        "df['embed_content_d2v'] = df['content'].apply(lambda x: embedd_d2v(model_d2v, x))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(df):\n",
        "    return 1 - spatial.distance.cosine(df[0], df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.37211666850418007"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['d2v_similarity'] = df[['embed_title_d2v', 'embed_content_d2v']].apply(cosine_similarity, axis = 1)\n",
        "df['d2v_similarity'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-08 12:42:30.145300: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.4937262279795504"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_USE = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "def embed_with_USE(embedder , x):\n",
        "    return embedder(x.split('.')).numpy().mean(axis = 0)\n",
        "\n",
        "df['embed_title_USE'] = df['title'].apply(lambda x: embed_with_USE(embed_USE, x))\n",
        "df['embed_content_USE'] = df['content'].apply(lambda x: embed_with_USE(embed_USE, x))\n",
        "df['USE_similarity'] = df[['embed_title_USE', 'embed_content_USE']].apply(cosine_similarity, axis = 1)\n",
        "df['USE_similarity'].mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5456832346447217"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_BERT = SentenceTransformer(r\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_with_USE(model , x):\n",
        "    return model.encode(x)\n",
        "\n",
        "df['embed_title_BERT'] = df['title'].apply(lambda x: embed_with_USE(model_BERT, x))\n",
        "df['embed_content_BERT'] = df['content'].apply(lambda x: embed_with_USE(model_BERT, x))\n",
        "df['BERT_similarity'] = df[['embed_title_BERT', 'embed_content_BERT']].apply(cosine_similarity, axis = 1)\n",
        "df['BERT_similarity'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.8.0\n",
            "Uninstalling tensorflow-2.8.0:\n",
            "  Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-cloud as it is not installed.\u001b[0m\n",
            "Collecting tensorflow==1.15\n",
            "  Using cached tensorflow-1.15.0-cp37-cp37m-macosx_10_11_x86_64.whl (124.0 MB)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (3.20.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.3)\n",
            "Collecting astor>=0.6.0\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
            "Processing /Users/max/Library/Caches/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3/gast-0.2.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.44.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (47.1.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
            "Installing collected packages: astor, tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "Successfully installed astor-0.8.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/Users/max/.pyenv/versions/3.7.11/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall tensorflow-cloud -y\n",
        "!pip install -U tensorflow==1.15\n",
        "!pip uninstall pytorch-lightning -y\n",
        "!pip uninstall tensorflow-probability -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")\n",
        "\n",
        "def create_elmo_embeddings(data):\n",
        "    return_ls = []\n",
        "    for dat in data.tolist():\n",
        "        embed=elmo.signatures['default']( tf.convert_to_tensor([dat]))[\"elmo\"]\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            sess.run(tf.tables_initializer())\n",
        "            out_x=sess.run(embed)\n",
        "            #out_y=ses.run(tf.reduce_mean(embed,1))\n",
        "        return_ls.append(out_x)\n",
        "    return return_ls\n",
        "\n",
        "temp1 = create_elmo_embeddings(df['content'][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor 'module_apply_default/aggregation/mul_3:0' shape=(2, 6, 1024) dtype=float32>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "elmo = tf_hub.Module(\"https://tfhub.dev/google/elmo/2\")\n",
        "embeddings = elmo(\n",
        "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/scipy/spatial/distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.31220945733240824"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "max_feat = 10000\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=5, max_features=max_feat, ngram_range=(1,3),\n",
        "                        strip_accents='unicode',\n",
        "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
        "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
        "                        stop_words = 'english')\n",
        "svdT = TruncatedSVD(n_components=390)\n",
        "tfidf.fit(df['content'])\n",
        "df['embed_content_tfidf'] = svdT.fit_transform(tfidf.transform(df['content'])).tolist()\n",
        "df['embed_title_tfidf'] = svdT.fit_transform(tfidf.transform(df['title'])).tolist()\n",
        "df['tfidf_similarity'] = df[['embed_title_tfidf', 'embed_content_tfidf']].apply(cosine_similarity, axis = 1)\n",
        "df['tfidf_similarity'].mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import os\n",
        "root_folder='.'\n",
        "data_folder_name='data'\n",
        "\n",
        "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
        "\n",
        "glove_filename='glove.6B.100d.txt'\n",
        "#glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
        "#glove_input_file = glove_filename\n",
        "#word2vec_output_file = glove_filename+'.word2vec'\n",
        "#glove2word2vec(glove_path, word2vec_output_file)\n",
        "\n",
        "# load the Stanford GloVe model\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/max/.pyenv/versions/3.7.11/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: Mean of empty slice.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8266875235549795"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "def embed_glove(s):\n",
        "    return_ls = []\n",
        "    for sentence in s:\n",
        "        array = []\n",
        "        for word in sentence.split():\n",
        "            try:\n",
        "                array.append(model.get_vector(word))\n",
        "            except:\n",
        "                pass\n",
        "        array = np.array(array)\n",
        "        #print(array.shape)\n",
        "        return_ls.append(array.mean(axis = 0))\n",
        "\n",
        "    return return_ls\n",
        "#Show a word embedding\n",
        "df['embed_content_glove'] = embed_glove(df['content'])\n",
        "df['embed_title_glove'] = embed_glove(df['title'])\n",
        "df['glove_similarity'] = df[['embed_title_glove', 'embed_content_glove']].apply(cosine_similarity, axis = 1)\n",
        "df['glove_similarity'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "sentences = df['content']\n",
        "sentences = [s.split() for s in sentences]\n",
        "model = FastText(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_fasttext(s):\n",
        "    return_ls = []\n",
        "    for sentence in s:\n",
        "        array = []\n",
        "        for word in sentence.split():\n",
        "            try:\n",
        "                array.append(model.wv[word])\n",
        "            except:\n",
        "                pass\n",
        "        array = np.array(array)\n",
        "        #print(array.shape)\n",
        "        return_ls.append(array.mean(axis = 0))\n",
        "\n",
        "    return return_ls\n",
        "\n",
        "df['embed_content_fasttext'] = embed_fasttext(df['content'])\n",
        "df['embed_title_fasttext'] = embed_fasttext(df['title'])\n",
        "df['fasttext_similarity'] = df[['embed_title_fasttext', 'embed_content_fasttext']].apply(cosine_similarity, axis = 1)\n",
        "df['fasttext_similarity'].mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
